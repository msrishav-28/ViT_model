{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msrishav-28/ViT_model/blob/main/Deepfake_Detection_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J37QBXr-8RyR",
        "outputId": "650c80a6-6f8c-4592-9bbb-2f997a42be9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: timm 1.0.15\n",
            "Uninstalling timm-1.0.15:\n",
            "  Successfully uninstalled timm-1.0.15\n",
            "\u001b[33mWARNING: Skipping facenet-pytorch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: albumentations 2.0.6\n",
            "Uninstalling albumentations-2.0.6:\n",
            "  Successfully uninstalled albumentations-2.0.6\n",
            "Found existing installation: opencv-python 4.11.0.86\n",
            "Uninstalling opencv-python-4.11.0.86:\n",
            "  Successfully uninstalled opencv-python-4.11.0.86\n",
            "\u001b[33mWARNING: Skipping pytorch-lightning as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: wandb 0.19.10\n",
            "Uninstalling wandb-0.19.10:\n",
            "  Successfully uninstalled wandb-0.19.10\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m432.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "dataproc-spark-connect 0.7.2 requires tqdm>=4.67, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m532.4/532.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "dataproc-spark-connect 0.7.2 requires tqdm>=4.67, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-0334d79cdd3f>\", line 18, in <cell line: 0>\n",
            "    import torchvision.transforms as T\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 6, in <module>\n",
            "    from torchvision import datasets, io, models, ops, transforms, utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
            "    from . import detection, optical_flow, quantization, segmentation, video\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
            "    from .faster_rcnn import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
            "    from .anchor_utils import AnchorGenerator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
            "    class AnchorGenerator(nn.Module):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
            "    device: torch.device = torch.device(\"cpu\"),\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(\"cpu\"),\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "`np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0334d79cdd3f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scriptable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_exportable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_scriptable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_exportable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_pretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_entrypoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mis_model_pretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_pretrained_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_pretrained_cfg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbyoanet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbyobnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcait\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/beit.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPatchEmbed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSwiGLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerNorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fused_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresample_patch_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_abs_pos_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_rel_pos_bias_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresolve_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve_model_data_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableImageDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAugMixDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_info\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCustomDatasetInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/readers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreader_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimg_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/readers/reader_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreader_image_folder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReaderImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreader_image_in_tar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReaderImageInTar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/readers/reader_image_folder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnatural_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_map\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_class_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_ema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelEma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelEmaV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mupdate_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_outdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/utils/summary.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtermsetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msdk\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwandb_sdk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_helper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhelper\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArtifact\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_alerts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlertLevel\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/artifacts/artifact.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArtifactCollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArtifactFiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRetryingClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/data_types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m from .sdk.data_types.base_types.media import (\n\u001b[1;32m     35\u001b[0m     \u001b[0mBatchableMedia\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/_dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0mNumberType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mNumberType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mNumberType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m     \u001b[0mNumberType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mNumberType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__expired_attributes__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    398\u001b[0m                 \u001b[0;34mf\"`np.{attr}` was removed in the NumPy 2.0 release. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;34mf\"{__expired_attributes__[attr]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead."
          ]
        }
      ],
      "source": [
        "# Create a fresh environment with compatible libraries for T4 GPU\n",
        "# First, clean up any potential conflicts\n",
        "!pip uninstall -y torch torchvision timm facenet-pytorch albumentations opencv-python pytorch-lightning wandb\n",
        "!pip cache purge\n",
        "\n",
        "# Install specific compatible versions optimized for T4 GPU\n",
        "!pip install -q torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q timm==0.9.7 einops==0.6.1 scikit-learn==1.2.2 matplotlib==3.7.1 seaborn==0.12.2 tqdm==4.65.0\n",
        "!pip install -q albumentations==1.3.1 facenet-pytorch==2.5.3 opencv-python==4.8.0.76 pytorchcv==0.0.67\n",
        "!pip install -q gdown==4.7.1 pytorch-lightning==2.0.9 wandb==0.15.11\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import timm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
        "from tqdm.auto import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# T4-specific optimizations\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TensorFloat-32 for faster matrix operations on T4\n",
        "torch.backends.cudnn.benchmark = True  # Optimize CUDNN for specific hardware\n",
        "torch.backends.cudnn.deterministic = False  # Allow non-deterministic algorithms for speed\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"üéâ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Run quick tensor operation to verify CUDA is working\n",
        "    test_tensor = torch.randn(1000, 1000, device=device)\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    result = torch.matmul(test_tensor, test_tensor)\n",
        "    end.record()\n",
        "\n",
        "    # Waits for everything to finish\n",
        "    torch.cuda.synchronize()\n",
        "    print(f\"CUDA test operation completed in {start.elapsed_time(end):.2f} ms\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Training will be extremely slow.\")\n",
        "\n",
        "# Clear memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, check if anything exists in the target directory\n",
        "!ls -la /content/drive\n",
        "\n",
        "# If there are files, remove them\n",
        "!rm -rf /content/drive/*\n",
        "\n",
        "# Try mounting again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories for our project\n",
        "!mkdir -p /content/drive/MyDrive/deepfake_detection/checkpoints\n",
        "!mkdir -p /content/drive/MyDrive/deepfake_detection/logs\n",
        "!mkdir -p /content/drive/MyDrive/datasets/faceforensics\n",
        "!mkdir -p /content/drive/MyDrive/datasets/celebdf\n",
        "\n",
        "print(\"‚úÖ Environment setup complete! T4 GPU optimizations enabled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkWqk1bVnJ4G",
        "outputId": "87a7336e-b1f5-4fdd-fdae-7b346e583848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive': No such file or directory\n",
            "Mounted at /content/drive\n",
            "‚úÖ Environment setup complete! T4 GPU optimizations enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the official FaceForensics++ script\n",
        "!wget -O /content/download.py https://github.com/ondyari/FaceForensics/raw/master/dataset/download.py\n",
        "\n",
        "print(\"‚úÖ Download script ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsfBm3hWom65",
        "outputId": "64f5bfd6-002e-4458-b8ac-2e5f75e5ca26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-05 17:42:39--  https://github.com/ondyari/FaceForensics/raw/master/dataset/download.py\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-05-05 17:42:39 ERROR 404: Not Found.\n",
            "\n",
            "‚úÖ Download script ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 500 original videos\n",
        "!python /content/download.py /content/drive/MyDrive/datasets/faceforensics --dataset original --compression c40 --type videos --server EU2 --num_videos 500\n",
        "\n",
        "# Download 500 Deepfakes videos\n",
        "!python /content/download.py /content/drive/MyDrive/datasets/faceforensics --dataset Deepfakes --compression c40 --type videos --server EU2 --num_videos 500\n",
        "\n",
        "# Download 500 Face2Face videos\n",
        "!python /content/download.py /content/drive/MyDrive/datasets/faceforensics --dataset Face2Face --compression c40 --type videos --server EU2 --num_videos 500\n",
        "\n",
        "# Download 500 NeuralTextures videos\n",
        "!python /content/download.py /content/drive/MyDrive/datasets/faceforensics --dataset NeuralTextures --compression c40 --type videos --server EU2 --num_videos 500\n",
        "\n",
        "print(\"‚úÖ FaceForensics++ subset downloaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QDrI1O7_rDP",
        "outputId": "bf061246-e3c6-4b15-a51a-07764df610c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FaceForensics++ subset downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CelebDF download\n",
        "print(\"Downloading CelebDF dataset...\")\n",
        "!gdown \"1iLx76wsbi9ztw6oAjk65MTA2mopR6y-8\" -O /content/drive/MyDrive/datasets/celebdf/celebdf.zip\n",
        "!unzip -q /content/drive/MyDrive/datasets/celebdf/celebdf.zip -d /content/drive/MyDrive/datasets/celebdf/\n",
        "!rm /content/drive/MyDrive/datasets/celebdf/celebdf.zip\n",
        "print(\"‚úÖ CelebDF download complete!\")"
      ],
      "metadata": {
        "id": "vIPRR16R_21G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76cb903-8997-4d04-b899-cb244d0bf5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CelebDF dataset...\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1iLx76wsbi9ztw6oAjk65MTA2mopR6y-8\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "unzip:  cannot find or open /content/drive/MyDrive/datasets/celebdf/celebdf.zip, /content/drive/MyDrive/datasets/celebdf/celebdf.zip.zip or /content/drive/MyDrive/datasets/celebdf/celebdf.zip.ZIP.\n",
            "rm: cannot remove '/content/drive/MyDrive/datasets/celebdf/celebdf.zip': No such file or directory\n",
            "‚úÖ CelebDF download complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import face detection model\n",
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "# Initialize the MTCNN detector\n",
        "face_detector = MTCNN(\n",
        "    image_size=224,\n",
        "    margin=40,\n",
        "    device=device,\n",
        "    keep_all=True,\n",
        "    post_process=True,\n",
        "    select_largest=False\n",
        ")\n",
        "\n",
        "def extract_faces_from_frame(frame, min_face_size=100, confidence_threshold=0.95):\n",
        "    \"\"\"Extract faces from a single frame using MTCNN\"\"\"\n",
        "    # Detect faces\n",
        "    try:\n",
        "        boxes, probs = face_detector.detect(frame, landmarks=False)\n",
        "\n",
        "        # Check if any faces were detected\n",
        "        if boxes is None:\n",
        "            return []\n",
        "\n",
        "        extracted_faces = []\n",
        "\n",
        "        # Process each detected face\n",
        "        for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
        "            # Filter by confidence and size\n",
        "            if prob < confidence_threshold:\n",
        "                continue\n",
        "\n",
        "            # Get coordinates\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "\n",
        "            # Filter small faces\n",
        "            if w < min_face_size or h < min_face_size:\n",
        "                continue\n",
        "\n",
        "            # Extract the face with margin\n",
        "            face = frame[max(0, y1):min(frame.shape[0], y2),\n",
        "                         max(0, x1):min(frame.shape[1], x2)]\n",
        "\n",
        "            # Resize to 224x224\n",
        "            if face.size > 0:  # Ensure face was extracted properly\n",
        "                face = cv2.resize(face, (224, 224))\n",
        "                extracted_faces.append(face)\n",
        "\n",
        "        return extracted_faces\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in face extraction: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_faces_from_video(video_path, max_frames=20, frame_interval=10):\n",
        "    \"\"\"Extract faces from video frames at regular intervals\"\"\"\n",
        "    try:\n",
        "        # Open video file\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Could not open video: {video_path}\")\n",
        "            return []\n",
        "\n",
        "        # Get total frames\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Check if video is too short\n",
        "        if total_frames < 10:  # Skip very short videos\n",
        "            cap.release()\n",
        "            return []\n",
        "\n",
        "        # Calculate frame indices to process\n",
        "        num_frames = min(max_frames, total_frames // frame_interval)\n",
        "        frame_indices = [i * frame_interval for i in range(num_frames)]\n",
        "\n",
        "        all_faces = []\n",
        "\n",
        "        # Process each frame\n",
        "        for frame_idx in frame_indices:\n",
        "            # Set frame position\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                continue\n",
        "\n",
        "            # Convert to RGB (from BGR)\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Extract faces\n",
        "            faces = extract_faces_from_frame(frame_rgb)\n",
        "            all_faces.extend(faces)\n",
        "\n",
        "            # Limit number of faces\n",
        "            if len(all_faces) >= max_frames:\n",
        "                all_faces = all_faces[:max_frames]\n",
        "                break\n",
        "\n",
        "        cap.release()\n",
        "        return all_faces\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"‚úÖ Face extraction utilities ready!\")"
      ],
      "metadata": {
        "id": "vYlaXnOd_5cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f59adb0-0a27-4bb7-a820-6379021ecd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CelebDF dataset...\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1iLx76wsbi9ztw6oAjk65MTA2mopR6y-8 \n",
            "\n",
            "unzip:  cannot find or open /content/drive/MyDrive/datasets/celebdf/celebdf.zip, /content/drive/MyDrive/datasets/celebdf/celebdf.zip.zip or /content/drive/MyDrive/datasets/celebdf/celebdf.zip.ZIP.\n",
            "rm: cannot remove '/content/drive/MyDrive/datasets/celebdf/celebdf.zip': No such file or directory\n",
            "‚úÖ CelebDF download complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceForensicsDataset(Dataset):\n",
        "    def __init__(self, root_dir, methods=['Deepfakes', 'Face2Face', 'NeuralTextures'],\n",
        "                 compression='c40', transform=None, max_faces_per_video=10,\n",
        "                 max_videos=None, use_extracted_faces=True):\n",
        "        \"\"\"\n",
        "        FaceForensics++ dataset with face extraction\n",
        "        Args:\n",
        "            root_dir: Root directory of the FaceForensics++ dataset\n",
        "            methods: List of manipulation methods to include\n",
        "            compression: Compression level (c40 recommended for Colab)\n",
        "            transform: Albumentations transforms\n",
        "            max_faces_per_video: Maximum number of face images to extract per video\n",
        "            max_videos: Maximum number of videos to use per class\n",
        "            use_extracted_faces: If True, extract faces from frames\n",
        "        \"\"\"\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.methods = methods\n",
        "        self.compression = compression\n",
        "        self.transform = transform\n",
        "        self.max_faces_per_video = max_faces_per_video\n",
        "        self.max_videos = max_videos\n",
        "        self.use_extracted_faces = use_extracted_faces\n",
        "\n",
        "        # Prepare paths and labels\n",
        "        self.samples = []  # Will contain (face_img_path, label, method_idx)\n",
        "\n",
        "        # Add original (real) videos\n",
        "        original_dir = self.root_dir / 'original_sequences/youtube' / compression / 'videos'\n",
        "        if original_dir.exists():\n",
        "            real_videos = list(original_dir.glob('*.mp4'))\n",
        "            if self.max_videos:\n",
        "                real_videos = real_videos[:self.max_videos]\n",
        "\n",
        "            for video_path in tqdm(real_videos, desc=\"Processing original videos\"):\n",
        "                faces = extract_faces_from_video(str(video_path), max_frames=self.max_faces_per_video)\n",
        "                for i, face in enumerate(faces):\n",
        "                    # Save face image\n",
        "                    face_filename = f\"{video_path.stem}_face{i}.jpg\"\n",
        "                    face_path = self.root_dir / 'extracted_faces' / 'original' / face_filename\n",
        "                    os.makedirs(face_path.parent, exist_ok=True)\n",
        "\n",
        "                    # Save face image if not exists\n",
        "                    if not face_path.exists():\n",
        "                        cv2.imwrite(str(face_path), cv2.cvtColor(face, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                    self.samples.append((str(face_path), 0, 0))  # 0 = real, 0 = method_idx for real\n",
        "\n",
        "        # Add manipulated videos\n",
        "        for method_idx, method in enumerate(self.methods, 1):  # Start from 1 (0 is reserved for real)\n",
        "            fake_dir = self.root_dir / f'manipulated_sequences/{method}' / compression / 'videos'\n",
        "            if fake_dir.exists():\n",
        "                fake_videos = list(fake_dir.glob('*.mp4'))\n",
        "                if self.max_videos:\n",
        "                    fake_videos = fake_videos[:self.max_videos]\n",
        "\n",
        "                for video_path in tqdm(fake_videos, desc=f\"Processing {method} videos\"):\n",
        "                    faces = extract_faces_from_video(str(video_path), max_frames=self.max_faces_per_video)\n",
        "                    for i, face in enumerate(faces):\n",
        "                        # Save face image\n",
        "                        face_filename = f\"{video_path.stem}_face{i}.jpg\"\n",
        "                        face_path = self.root_dir / 'extracted_faces' / method / face_filename\n",
        "                        os.makedirs(face_path.parent, exist_ok=True)\n",
        "\n",
        "                        # Save face image if not exists\n",
        "                        if not face_path.exists():\n",
        "                            cv2.imwrite(str(face_path), cv2.cvtColor(face, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                        self.samples.append((str(face_path), 1, method_idx))  # 1 = fake\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        face_path, label, method_idx = self.samples[idx]\n",
        "\n",
        "        # Load face image\n",
        "        image = cv2.imread(face_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        return image, label, method_idx\n",
        "\n",
        "class CelebDFDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, max_faces_per_video=10, max_videos=None):\n",
        "        \"\"\"\n",
        "        CelebDF dataset with face extraction\n",
        "        Args:\n",
        "            root_dir: Root directory of the CelebDF dataset\n",
        "            transform: Albumentations transforms\n",
        "            max_faces_per_video: Maximum number of face images to extract per video\n",
        "            max_videos: Maximum number of videos to use per class\n",
        "        \"\"\"\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.max_faces_per_video = max_faces_per_video\n",
        "        self.max_videos = max_videos\n",
        "\n",
        "        # Prepare paths and labels\n",
        "        self.samples = []  # Will contain (face_img_path, label)\n",
        "\n",
        "        # Celeb-real videos (real)\n",
        "        real_dir = self.root_dir / 'Celeb-real'\n",
        "        if real_dir.exists():\n",
        "            real_videos = list(real_dir.glob('*.mp4'))\n",
        "            if self.max_videos:\n",
        "                real_videos = real_videos[:self.max_videos]\n",
        "\n",
        "            for video_path in tqdm(real_videos, desc=\"Processing CelebDF real videos\"):\n",
        "                faces = extract_faces_from_video(str(video_path), max_frames=self.max_faces_per_video)\n",
        "                for i, face in enumerate(faces):\n",
        "                    # Save face image\n",
        "                    face_filename = f\"{video_path.stem}_face{i}.jpg\"\n",
        "                    face_path = self.root_dir / 'extracted_faces' / 'real' / face_filename\n",
        "                    os.makedirs(face_path.parent, exist_ok=True)\n",
        "\n",
        "                    # Save face image if not exists\n",
        "                    if not face_path.exists():\n",
        "                        cv2.imwrite(str(face_path), cv2.cvtColor(face, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                    self.samples.append((str(face_path), 0))  # 0 = real\n",
        "\n",
        "        # Celeb-synthesis videos (fake)\n",
        "        fake_dir = self.root_dir / 'Celeb-synthesis'\n",
        "        if fake_dir.exists():\n",
        "            fake_videos = list(fake_dir.glob('*.mp4'))\n",
        "            if self.max_videos:\n",
        "                fake_videos = fake_videos[:self.max_videos]\n",
        "\n",
        "            for video_path in tqdm(fake_videos, desc=\"Processing CelebDF fake videos\"):\n",
        "                faces = extract_faces_from_video(str(video_path), max_frames=self.max_faces_per_video)\n",
        "                for i, face in enumerate(faces):\n",
        "                    # Save face image\n",
        "                    face_filename = f\"{video_path.stem}_face{i}.jpg\"\n",
        "                    face_path = self.root_dir / 'extracted_faces' / 'fake' / face_filename\n",
        "                    os.makedirs(face_path.parent, exist_ok=True)\n",
        "\n",
        "                    # Save face image if not exists\n",
        "                    if not face_path.exists():\n",
        "                        cv2.imwrite(str(face_path), cv2.cvtColor(face, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                    self.samples.append((str(face_path), 1))  # 1 = fake\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        face_path, label = self.samples[idx]\n",
        "\n",
        "        # Load face image\n",
        "        image = cv2.imread(face_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        return image, label, -1  # -1 as method_idx placeholder (not used for CelebDF)\n",
        "\n",
        "class CombinedDataset(Dataset):\n",
        "    def __init__(self, ff_dataset=None, celebdf_dataset=None, balance=True):\n",
        "        \"\"\"\n",
        "        Combines FaceForensics++ and CelebDF datasets\n",
        "        Args:\n",
        "            ff_dataset: FaceForensicsDataset\n",
        "            celebdf_dataset: CelebDFDataset\n",
        "            balance: If True, balance classes by undersampling\n",
        "        \"\"\"\n",
        "        self.ff_dataset = ff_dataset\n",
        "        self.celebdf_dataset = celebdf_dataset\n",
        "\n",
        "        if ff_dataset is None and celebdf_dataset is None:\n",
        "            raise ValueError(\"At least one dataset must be provided\")\n",
        "\n",
        "        # Combine samples\n",
        "        self.samples = []\n",
        "\n",
        "        # Add FaceForensics++ samples\n",
        "        if ff_dataset is not None:\n",
        "            for i in range(len(ff_dataset)):\n",
        "                self.samples.append(('ff', i))\n",
        "\n",
        "        # Add CelebDF samples\n",
        "        if celebdf_dataset is not None:\n",
        "            for i in range(len(celebdf_dataset)):\n",
        "                self.samples.append(('celebdf', i))\n",
        "\n",
        "        # Balance if needed\n",
        "        if balance and ff_dataset is not None and celebdf_dataset is not None:\n",
        "            self._balance_samples()\n",
        "\n",
        "    def _balance_samples(self):\n",
        "        # Count real and fake samples from each dataset\n",
        "        ff_real, ff_fake = 0, 0\n",
        "        for i in range(len(self.ff_dataset)):\n",
        "            _, label, _ = self.ff_dataset[i]\n",
        "            if label == 0:\n",
        "                ff_real += 1\n",
        "            else:\n",
        "                ff_fake += 1\n",
        "\n",
        "        celebdf_real, celebdf_fake = 0, 0\n",
        "        for i in range(len(self.celebdf_dataset)):\n",
        "            _, label, _ = self.celebdf_dataset[i]\n",
        "            if label == 0:\n",
        "                celebdf_real += 1\n",
        "            else:\n",
        "                celebdf_fake += 1\n",
        "\n",
        "        # Create balanced samples\n",
        "        real_samples = []\n",
        "        fake_samples = []\n",
        "\n",
        "        for sample in self.samples:\n",
        "            dataset, idx = sample\n",
        "            if dataset == 'ff':\n",
        "                _, label, _ = self.ff_dataset[idx]\n",
        "            else:\n",
        "                _, label, _ = self.celebdf_dataset[idx]\n",
        "\n",
        "            if label == 0:\n",
        "                real_samples.append(sample)\n",
        "            else:\n",
        "                fake_samples.append(sample)\n",
        "\n",
        "        # Undersample\n",
        "        target_num_real = min(len(real_samples), len(fake_samples))\n",
        "        target_num_fake = min(len(real_samples), len(fake_samples))\n",
        "\n",
        "        real_samples = random.sample(real_samples, target_num_real)\n",
        "        fake_samples = random.sample(fake_samples, target_num_fake)\n",
        "\n",
        "        # Update samples\n",
        "        self.samples = real_samples + fake_samples\n",
        "        random.shuffle(self.samples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dataset, sample_idx = self.samples[idx]\n",
        "\n",
        "        if dataset == 'ff':\n",
        "            return self.ff_dataset[sample_idx]\n",
        "        else:\n",
        "            return self.celebdf_dataset[sample_idx]\n",
        "\n",
        "print(\"‚úÖ Dataset classes defined!\")"
      ],
      "metadata": {
        "id": "7578bnBV_75R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_augmentation_pipeline(is_train=True):\n",
        "    \"\"\"Create augmentation pipeline optimized for deepfake detection\"\"\"\n",
        "\n",
        "    if is_train:\n",
        "        return A.Compose([\n",
        "            # Face-specific augmentations\n",
        "            A.RandomResizedCrop(224, 224, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "\n",
        "            # Color augmentations to handle different video qualities\n",
        "            A.OneOf([\n",
        "                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.7),\n",
        "                A.ToGray(p=0.3),\n",
        "            ], p=0.8),\n",
        "\n",
        "            # Compression artifacts simulation\n",
        "            A.OneOf([\n",
        "                A.ImageCompression(quality_lower=60, quality_upper=100, p=0.5),\n",
        "                A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
        "            ], p=0.3),\n",
        "\n",
        "            # Noise augmentations\n",
        "            A.OneOf([\n",
        "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
        "                A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.5),\n",
        "            ], p=0.3),\n",
        "\n",
        "            # Normalize and convert to tensor\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(224, 224),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "print(\"‚úÖ Augmentation pipeline defined!\")"
      ],
      "metadata": {
        "id": "7o4RZJnp_-Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeDetector(nn.Module):\n",
        "    def __init__(self, num_classes=2, num_manipulation_types=4,\n",
        "                 pretrained=True, dropout=0.3, model_name='vit_base_patch16_224'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ViT-Base\n",
        "        self.backbone = timm.create_model(model_name,\n",
        "                                        pretrained=pretrained,\n",
        "                                        num_classes=0)  # Remove classification head\n",
        "\n",
        "        # Get feature dimension\n",
        "        feature_dim = self.backbone.num_features\n",
        "\n",
        "        # Classification heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Binary classification head (real/fake)\n",
        "        self.binary_classifier = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # Manipulation type classification head\n",
        "        self.manipulation_classifier = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, num_manipulation_types)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        # Extract features\n",
        "        features = self.backbone(x)\n",
        "        features = self.dropout(features)\n",
        "\n",
        "        # Binary classification\n",
        "        binary_logits = self.binary_classifier(features)\n",
        "\n",
        "        # Manipulation type classification\n",
        "        manipulation_logits = self.manipulation_classifier(features)\n",
        "\n",
        "        if return_features:\n",
        "            return binary_logits, manipulation_logits, features\n",
        "        else:\n",
        "            return binary_logits, manipulation_logits\n",
        "\n",
        "print(\"‚úÖ Model architecture defined!\")"
      ],
      "metadata": {
        "id": "SwIhW-vKABAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion_binary,\n",
        "                criterion_manipulation, scaler, device):\n",
        "    \"\"\"Train for one epoch with mixed precision\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_binary = 0\n",
        "    correct_manipulation = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
        "    for images, labels, manipulation_types in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        manipulation_types = manipulation_types.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training\n",
        "        with autocast():\n",
        "            binary_logits, manipulation_logits = model(images)\n",
        "            loss_binary = criterion_binary(binary_logits, labels)\n",
        "\n",
        "            # Only compute manipulation loss for FaceForensics samples\n",
        "            valid_manip = manipulation_types >= 0\n",
        "            if valid_manip.sum() > 0:\n",
        "                loss_manipulation = criterion_manipulation(\n",
        "                    manipulation_logits[valid_manip],\n",
        "                    manipulation_types[valid_manip]\n",
        "                )\n",
        "                loss = loss_binary + 0.5 * loss_manipulation\n",
        "            else:\n",
        "                loss = loss_binary\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted_binary = binary_logits.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct_binary += predicted_binary.eq(labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{running_loss/(pbar.n+1):.4f}',\n",
        "            'acc': f'{100.*correct_binary/total:.2f}%',\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc_binary = 100. * correct_binary / total\n",
        "\n",
        "    return epoch_loss, epoch_acc_binary\n",
        "\n",
        "def validate(model, val_loader, criterion_binary, criterion_manipulation, device):\n",
        "    \"\"\"Validation function\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, manipulation_types in tqdm(val_loader, desc='Validation', leave=False):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            manipulation_types = manipulation_types.to(device)\n",
        "\n",
        "            binary_logits, manipulation_logits = model(images)\n",
        "            loss_binary = criterion_binary(binary_logits, labels)\n",
        "\n",
        "            # Only compute manipulation loss for FaceForensics samples\n",
        "            valid_manip = manipulation_types >= 0\n",
        "            if valid_manip.sum() > 0:\n",
        "                loss_manipulation = criterion_manipulation(\n",
        "                    manipulation_logits[valid_manip],\n",
        "                    manipulation_types[valid_manip]\n",
        "                )\n",
        "                loss = loss_binary + 0.5 * loss_manipulation\n",
        "            else:\n",
        "                loss = loss_binary\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Store predictions for metrics\n",
        "            probabilities = F.softmax(binary_logits, dim=1)[:, 1]\n",
        "            _, predicted = binary_logits.max(1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_acc = 100. * np.mean(np.array(all_labels) == np.array(all_predictions))\n",
        "\n",
        "    # ROC AUC\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probabilities)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    return val_loss, val_acc, roc_auc, all_labels, all_predictions, all_probabilities\n",
        "\n",
        "print(\"‚úÖ Training utilities defined!\")"
      ],
      "metadata": {
        "id": "gr6hl_fnAEB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(labels, predictions, classes=['Real', 'Fake']):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('/content/drive/MyDrive/deepfake_detection/confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(labels, probabilities):\n",
        "    \"\"\"Plot ROC curve\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(labels, probabilities)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig('/content/drive/MyDrive/deepfake_detection/roc_curve.png')\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualization functions defined!\")"
      ],
      "metadata": {
        "id": "kWvmWqxUAGKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'batch_size': 32,  # Adjust based on GPU memory\n",
        "        'num_epochs': 30,\n",
        "        'learning_rate': 1e-4,\n",
        "        'weight_decay': 0.01,\n",
        "        'num_workers': 2,  # Colab works best with 2 workers\n",
        "        'patience': 7,\n",
        "        'mixed_precision': True,\n",
        "        'gradient_accumulation_steps': 2,  # For larger effective batch size\n",
        "        'max_videos_per_class': 500,  # Set to 500 for all methods\n",
        "        'max_faces_per_video': 10,\n",
        "        'train_val_split': 0.8,  # 80% training, 20% validation\n",
        "    }\n",
        "\n",
        "    # Create augmentation pipelines\n",
        "    train_transform = get_augmentation_pipeline(is_train=True)\n",
        "    val_transform = get_augmentation_pipeline(is_train=False)\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"Loading FaceForensics++ dataset...\")\n",
        "    ff_dataset = FaceForensicsDataset(\n",
        "        root_dir='/content/drive/MyDrive/datasets/faceforensics',\n",
        "        methods=['Deepfakes', 'Face2Face', 'NeuralTextures'],  # Include Face2Face\n",
        "        compression='c40',\n",
        "        transform=None,  # Will be applied later\n",
        "        max_faces_per_video=config['max_faces_per_video'],\n",
        "        max_videos=config['max_videos_per_class']\n",
        "    )\n",
        "\n",
        "    print(\"Loading CelebDF dataset...\")\n",
        "    celebdf_dataset = CelebDFDataset(\n",
        "        root_dir='/content/drive/MyDrive/datasets/celebdf',\n",
        "        transform=None,  # Will be applied later\n",
        "        max_faces_per_video=config['max_faces_per_video'],\n",
        "        max_videos=config['max_videos_per_class']\n",
        "    )\n",
        "\n",
        "    # Split datasets into train and validation\n",
        "    ff_samples = ff_dataset.samples\n",
        "    celebdf_samples = celebdf_dataset.samples\n",
        "\n",
        "    random.shuffle(ff_samples)\n",
        "    random.shuffle(celebdf_samples)\n",
        "\n",
        "    train_ff_samples = ff_samples[:int(len(ff_samples) * config['train_val_split'])]\n",
        "    val_ff_samples = ff_samples[int(len(ff_samples) * config['train_val_split']):]\n",
        "\n",
        "    train_celebdf_samples = celebdf_samples[:int(len(celebdf_samples) * config['train_val_split'])]\n",
        "    val_celebdf_samples = celebdf_samples[int(len(celebdf_samples) * config['train_val_split']):]\n",
        "\n",
        "    # Create train and validation datasets with proper transforms\n",
        "    train_ff_dataset = FaceForensicsDataset(root_dir='/content/drive/MyDrive/datasets/faceforensics')\n",
        "    train_ff_dataset.samples = train_ff_samples\n",
        "    train_ff_dataset.transform = train_transform\n",
        "\n",
        "    val_ff_dataset = FaceForensicsDataset(root_dir='/content/drive/MyDrive/datasets/faceforensics')\n",
        "    val_ff_dataset.samples = val_ff_samples\n",
        "    val_ff_dataset.transform = val_transform\n",
        "\n",
        "    train_celebdf_dataset = CelebDFDataset(root_dir='/content/drive/MyDrive/datasets/celebdf')\n",
        "    train_celebdf_dataset.samples = train_celebdf_samples\n",
        "    train_celebdf_dataset.transform = train_transform\n",
        "\n",
        "    val_celebdf_dataset = CelebDFDataset(root_dir='/content/drive/MyDrive/datasets/celebdf')\n",
        "    val_celebdf_dataset.samples = val_celebdf_samples\n",
        "    val_celebdf_dataset.transform = val_transform\n",
        "\n",
        "    # Combine datasets\n",
        "    train_dataset = CombinedDataset(train_ff_dataset, train_celebdf_dataset, balance=True)\n",
        "    val_dataset = CombinedDataset(val_ff_dataset, val_celebdf_dataset, balance=True)\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=config['num_workers'],\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=config['num_workers'],\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = DeepfakeDetector(\n",
        "        num_classes=2,  # Binary: real/fake\n",
        "        num_manipulation_types=4,  # Real, Deepfakes, Face2Face, NeuralTextures\n",
        "        pretrained=True,\n",
        "        dropout=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_binary = nn.CrossEntropyLoss()\n",
        "    criterion_manipulation = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer with weight decay\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=config['num_epochs']\n",
        "    )\n",
        "\n",
        "    # Mixed precision scaler\n",
        "    scaler = GradScaler() if config['mixed_precision'] else None\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(patience=config['patience'])\n",
        "\n",
        "    # Training metrics\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "    val_aucs = []\n",
        "    best_val_auc = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, optimizer, criterion_binary,\n",
        "            criterion_manipulation, scaler, device\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, val_auc, val_labels, val_predictions, val_probabilities = validate(\n",
        "            model, val_loader, criterion_binary, criterion_manipulation, device\n",
        "        )\n",
        "\n",
        "        # Step scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        val_aucs.append(val_auc)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_auc': val_auc,\n",
        "                'val_acc': val_acc,\n",
        "            }, '/content/drive/MyDrive/deepfake_detection/checkpoints/best_model.pth')\n",
        "            print(f\"Saved best model with AUC: {val_auc:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    # Plot training curves\n",
        "    plot_training_curves(train_losses, val_losses, train_accs, val_accs, val_aucs)\n",
        "\n",
        "    # Plot final evaluation metrics\n",
        "    plot_confusion_matrix(val_labels, val_predictions)\n",
        "    plot_roc_curve(val_labels, val_probabilities)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(val_labels, val_predictions,\n",
        "                               target_names=['Real', 'Fake']))\n",
        "\n",
        "    print(f\"\\nTraining completed! Best validation AUC: {best_val_auc:.4f}\")\n",
        "\n",
        "print(\"‚úÖ Main training function defined!\")"
      ],
      "metadata": {
        "id": "E8OMMNY2AIpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model_path, image_path, device):\n",
        "    \"\"\"Inference on a single image\"\"\"\n",
        "    # Load model\n",
        "    model = DeepfakeDetector(num_classes=2, num_manipulation_types=4)  # Updated for 4 types\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    transform = get_augmentation_pipeline(is_train=False)\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Extract face if needed\n",
        "    faces = extract_faces_from_frame(image)\n",
        "    if len(faces) == 0:\n",
        "        print(\"No face detected in the image!\")\n",
        "        return None\n",
        "\n",
        "    # Use the first face\n",
        "    face = faces[0]\n",
        "\n",
        "    # Apply transformations\n",
        "    augmented = transform(image=face)\n",
        "    image_tensor = augmented['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        binary_logits, manipulation_logits = model(image_tensor)\n",
        "        binary_probs = F.softmax(binary_logits, dim=1)\n",
        "        manipulation_probs = F.softmax(manipulation_logits, dim=1)\n",
        "\n",
        "        is_fake = binary_probs[0][1] > 0.5\n",
        "        confidence = binary_probs[0][1].item() if is_fake else binary_probs[0][0].item()\n",
        "\n",
        "        manipulation_idx = manipulation_probs[0].argmax().item()\n",
        "        manipulation_types = ['Real', 'Deepfakes', 'Face2Face', 'NeuralTextures']  # Updated list\n",
        "        manipulation_type = manipulation_types[manipulation_idx]\n",
        "\n",
        "    result = {\n",
        "        'is_fake': is_fake,\n",
        "        'confidence': confidence,\n",
        "        'manipulation_type': manipulation_type if is_fake else 'N/A',\n",
        "        'manipulation_confidence': manipulation_probs[0][manipulation_idx].item()\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "def visualize_result(image_path, result):\n",
        "    \"\"\"Visualize inference result\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Extract face\n",
        "    faces = extract_faces_from_frame(image)\n",
        "    if len(faces) == 0:\n",
        "        print(\"No face detected in the image!\")\n",
        "        return\n",
        "\n",
        "    face = faces[0]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Display face\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(face)\n",
        "    plt.title(\"Input Face\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display result\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    # Create a simple gauge chart\n",
        "    fake_prob = result['confidence'] if result['is_fake'] else 1 - result['confidence']\n",
        "\n",
        "    # Create gauge chart\n",
        "    fig, ax = plt.subplot_mosaic([['gauge']], figsize=(6, 6))\n",
        "\n",
        "    # Set up a basic gauge\n",
        "    gauge = plt.Circle((0.5, 0.5), 0.4, color='lightgray', fill=True)\n",
        "    ax['gauge'].add_artist(gauge)\n",
        "\n",
        "    # Add a \"needle\" showing the probability\n",
        "    angle = (1 - fake_prob) * np.pi\n",
        "    x = 0.5 + 0.35 * np.cos(angle)\n",
        "    y = 0.5 + 0.35 * np.sin(angle)\n",
        "    ax['gauge'].plot([0.5, x], [0.5, y], color='red', linewidth=3)\n",
        "\n",
        "    # Add gauge labels\n",
        "    ax['gauge'].text(0.15, 0.5, \"REAL\", fontsize=14, ha='center', va='center', color='green')\n",
        "    ax['gauge'].text(0.85, 0.5, \"FAKE\", fontsize=14, ha='center', va='center', color='red')\n",
        "\n",
        "    # Add confidence text\n",
        "    if result['is_fake']:\n",
        "        text = f\"FAKE ({fake_prob:.1%} confidence)\\nType: {result['manipulation_type']}\"\n",
        "        color = 'red'\n",
        "    else:\n",
        "        text = f\"REAL ({(1-fake_prob):.1%} confidence)\"\n",
        "        color = 'green'\n",
        "\n",
        "    ax['gauge'].text(0.5, 0.25, text, fontsize=16, ha='center', va='center',\n",
        "                   color=color, weight='bold')\n",
        "\n",
        "    ax['gauge'].set_xlim(0, 1)\n",
        "    ax['gauge'].set_ylim(0, 1)\n",
        "    ax['gauge'].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/deepfake_detection/result.png')\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Inference functions defined!\")"
      ],
      "metadata": {
        "id": "2eBnUtSLOj6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model_path, image_path, device):\n",
        "    \"\"\"Inference on a single image\"\"\"\n",
        "    # Load model\n",
        "    model = DeepfakeDetector(num_classes=2, num_manipulation_types=3)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    transform = get_augmentation_pipeline(is_train=False)\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Extract face if needed\n",
        "    faces = extract_faces_from_frame(image)\n",
        "    if len(faces) == 0:\n",
        "        print(\"No face detected in the image!\")\n",
        "        return None\n",
        "\n",
        "    # Use the first face\n",
        "    face = faces[0]\n",
        "\n",
        "    # Apply transformations\n",
        "    augmented = transform(image=face)\n",
        "    image_tensor = augmented['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        binary_logits, manipulation_logits = model(image_tensor)\n",
        "        binary_probs = F.softmax(binary_logits, dim=1)\n",
        "        manipulation_probs = F.softmax(manipulation_logits, dim=1)\n",
        "\n",
        "        is_fake = binary_probs[0][1] > 0.5\n",
        "        confidence = binary_probs[0][1].item() if is_fake else binary_probs[0][0].item()\n",
        "\n",
        "        manipulation_idx = manipulation_probs[0].argmax().item()\n",
        "        manipulation_types = ['Real', 'Deepfakes', 'NeuralTextures']\n",
        "        manipulation_type = manipulation_types[manipulation_idx]\n",
        "\n",
        "    result = {\n",
        "        'is_fake': is_fake,\n",
        "        'confidence': confidence,\n",
        "        'manipulation_type': manipulation_type if is_fake else 'N/A',\n",
        "        'manipulation_confidence': manipulation_probs[0][manipulation_idx].item()\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "def visualize_result(image_path, result):\n",
        "    \"\"\"Visualize inference result\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Extract face\n",
        "    faces = extract_faces_from_frame(image)\n",
        "    if len(faces) == 0:\n",
        "        print(\"No face detected in the image!\")\n",
        "        return\n",
        "\n",
        "    face = faces[0]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Display face\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(face)\n",
        "    plt.title(\"Input Face\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display result\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    # Create a simple gauge chart\n",
        "    fake_prob = result['confidence'] if result['is_fake'] else 1 - result['confidence']\n",
        "\n",
        "    # Create gauge chart\n",
        "    fig, ax = plt.subplot_mosaic([['gauge']], figsize=(6, 6))\n",
        "\n",
        "    # Set up a basic gauge\n",
        "    gauge = plt.Circle((0.5, 0.5), 0.4, color='lightgray', fill=True)\n",
        "    ax['gauge'].add_artist(gauge)\n",
        "\n",
        "    # Add a \"needle\" showing the probability\n",
        "    angle = (1 - fake_prob) * np.pi\n",
        "    x = 0.5 + 0.35 * np.cos(angle)\n",
        "    y = 0.5 + 0.35 * np.sin(angle)\n",
        "    ax['gauge'].plot([0.5, x], [0.5, y], color='red', linewidth=3)\n",
        "\n",
        "    # Add gauge labels\n",
        "    ax['gauge'].text(0.15, 0.5, \"REAL\", fontsize=14, ha='center', va='center', color='green')\n",
        "    ax['gauge'].text(0.85, 0.5, \"FAKE\", fontsize=14, ha='center', va='center', color='red')\n",
        "\n",
        "    # Add confidence text\n",
        "    if result['is_fake']:\n",
        "        text = f\"FAKE ({fake_prob:.1%} confidence)\\nType: {result['manipulation_type']}\"\n",
        "        color = 'red'\n",
        "    else:\n",
        "        text = f\"REAL ({(1-fake_prob):.1%} confidence)\"\n",
        "        color = 'green'\n",
        "\n",
        "    ax['gauge'].text(0.5, 0.25, text, fontsize=16, ha='center', va='center',\n",
        "                   color=color, weight='bold')\n",
        "\n",
        "    ax['gauge'].set_xlim(0, 1)\n",
        "    ax['gauge'].set_ylim(0, 1)\n",
        "    ax['gauge'].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/deepfake_detection/result.png')\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Inference functions defined!\")"
      ],
      "metadata": {
        "id": "Hb97CkZcPhRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Start training\n",
        "    main()"
      ],
      "metadata": {
        "id": "eXgFOvu0PnLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference on a test image\n",
        "def test_on_image(image_path):\n",
        "    \"\"\"Test the model on a sample image\"\"\"\n",
        "    model_path = '/content/drive/MyDrive/deepfake_detection/checkpoints/best_model.pth'\n",
        "\n",
        "    # Check if model exists\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Model not found! Please train the model first.\")\n",
        "        return\n",
        "\n",
        "    # Run inference\n",
        "    result = inference(model_path, image_path, device)\n",
        "\n",
        "    if result is None:\n",
        "        print(\"Could not analyze the image.\")\n",
        "        return\n",
        "\n",
        "    # Print result\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Result: {'FAKE' if result['is_fake'] else 'REAL'}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "\n",
        "    if result['is_fake']:\n",
        "        print(f\"Manipulation type: {result['manipulation_type']}\")\n",
        "        print(f\"Manipulation confidence: {result['manipulation_confidence']:.2%}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Visualize\n",
        "    visualize_result(image_path, result)\n",
        "\n",
        "# To test on your own image, uncomment and provide a path:\n",
        "# test_on_image('/content/drive/MyDrive/your_test_image.jpg')"
      ],
      "metadata": {
        "id": "IysEs72tPpFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}